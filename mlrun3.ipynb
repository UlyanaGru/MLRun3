{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "163fb5c8",
   "metadata": {},
   "source": [
    "Step 1: Tabular Q-learning with SoftMax policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7ac9722",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade1e963",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_softmax(values: np.ndarray, T=1.):\n",
    "    # Subtract max for numerical stability\n",
    "    shifted_values = values - np.max(values)\n",
    "    exp_values = np.exp(shifted_values / T)\n",
    "    probas = exp_values / np.sum(exp_values)\n",
    "    return probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6baab960",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m test_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m5\u001b[39m]:\n\u001b[1;32m----> 4\u001b[0m   local_softmax \u001b[38;5;241m=\u001b[39m \u001b[43mmy_softmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m      6\u001b[0m     torch_softmax \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(torch\u001b[38;5;241m.\u001b[39mfrom_numpy(test_data)\u001b[38;5;241m/\u001b[39mtemp, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m, in \u001b[0;36mmy_softmax\u001b[1;34m(values, T)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmy_softmax\u001b[39m(values: np\u001b[38;5;241m.\u001b[39mndarray, T\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.\u001b[39m):\n\u001b[0;32m      2\u001b[0m     probas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m probas \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m probas\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "test_data = np.arange(5)\n",
    "for temp in [0.1, 0.5, 1, 5]:\n",
    "  local_softmax = my_softmax(test_data, T=temp)\n",
    "  with torch.no_grad():\n",
    "    torch_softmax = torch.softmax(torch.from_numpy(test_data)/temp, dim=-1)\n",
    "  assert np.allclose(local_softmax, torch_softmax.numpy())\n",
    "  print(f'Passed for temp={temp}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb66f488",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = 1\n",
    "plt.bar(test_data, my_softmax(test_data, T=temp))\n",
    "plt.title(f'Example softmax with temp = {temp}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88daab4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, alpha, discount, get_legal_actions, temp=1.):\n",
    "        \"\"\"\n",
    "        Q-Learning Agent\n",
    "        based on https://inst.eecs.berkeley.edu/~cs188/sp19/projects.html\n",
    "        Instance variables you have access to\n",
    "          - self.epsilon (exploration prob)\n",
    "          - self.alpha (learning rate)\n",
    "          - self.discount (discount rate aka gamma)\n",
    "\n",
    "        Functions you should use\n",
    "          - self.get_legal_actions(state) {state, hashable -> list of actions, each is hashable}\n",
    "            which returns legal actions for a state\n",
    "          - self.get_qvalue(state,action)\n",
    "            which returns Q(state,action)\n",
    "          - self.set_qvalue(state,action,value)\n",
    "            which sets Q(state,action) := value\n",
    "        !!!Important!!!\n",
    "        Note: please avoid using self._qValues directly.\n",
    "            There's a special self.get_qvalue/set_qvalue for that.\n",
    "        \"\"\"\n",
    "\n",
    "        self.get_legal_actions = get_legal_actions\n",
    "        self._qvalues = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "        self.alpha = alpha\n",
    "        self.discount = discount\n",
    "        self.temp = temp\n",
    "\n",
    "    def get_qvalue(self, state, action):\n",
    "        \"\"\"Returns Q(state,action)\"\"\"\n",
    "        return self._qvalues[state][action]\n",
    "\n",
    "    def set_qvalue(self, state, action, value):\n",
    "        \"\"\"Sets the Qvalue for [state,action] to the given value\"\"\"\n",
    "        self._qvalues[state][action] = value\n",
    "\n",
    "    def get_value(self, state):\n",
    "        \"\"\"\n",
    "        Compute your agent's estimate of V(s) using current q-values\n",
    "        V(s) = max_over_action Q(state,action) over possible actions.\n",
    "        Note: please take into account that q-values can be negative.\n",
    "        \"\"\"\n",
    "        possible_actions = self.get_legal_actions(state)\n",
    "\n",
    "        # If there are no legal actions, return 0.0\n",
    "        if len(possible_actions) == 0:\n",
    "            return 0.0\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        # Calculate the approximation of value function V(s).\n",
    "        q_values = [self.get_qvalue(state, action) for action in possible_actions]\n",
    "        value = np.max(q_values)\n",
    "        value = None\n",
    "        assert value is not None\n",
    "\n",
    "        return value\n",
    "\n",
    "    def update(self, state, action, reward, next_state):\n",
    "        \"\"\"\n",
    "        You should do your Q-Value update here:\n",
    "           Q(s,a) := (1 - alpha) * Q(s,a) + alpha * (r + gamma * V(s'))\n",
    "        \"\"\"\n",
    "\n",
    "        # agent parameters\n",
    "        gamma = self.discount\n",
    "        learning_rate = self.alpha\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        # Calculate the updated value of Q(s, a).\n",
    "        qvalue = None\n",
    "        assert qvalue is not None\n",
    "\n",
    "        self.set_qvalue(state, action, qvalue)\n",
    "\n",
    "    def get_best_action(self, state):\n",
    "        \"\"\"\n",
    "        Compute the best action to take in a state (using current q-values).\n",
    "        \"\"\"\n",
    "        possible_actions = self.get_legal_actions(state)\n",
    "\n",
    "        # If there are no legal actions, return None\n",
    "        if len(possible_actions) == 0:\n",
    "            return None\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        # Choose the best action wrt the qvalues.\n",
    "        best_action = None\n",
    "        assert best_action is not None\n",
    "\n",
    "        return best_action\n",
    "\n",
    "    def get_softmax_policy(self, state):\n",
    "        \"\"\"\n",
    "        Compute all actions probabilities in the current state according\n",
    "        to their q-values using softmax policy.\n",
    "\n",
    "        Actions probability should be computed as\n",
    "        p(a_i|s) = softmax([q(s, a_1), q(s, a_2), ... q(s, a_k)])_i\n",
    "        Softmax temperature is set to `self.temp`.\n",
    "        See the formula in the notebook for more details\n",
    "        \"\"\"\n",
    "        possible_actions = self.get_legal_actions(state)\n",
    "        # If there are no legal actions, return None\n",
    "        if len(possible_actions) == 0:\n",
    "            return None\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        # Compute all actions probabilities in the current state using softmax\n",
    "        q_values = None\n",
    "        assert q_values is not None\n",
    "        probabilities = None\n",
    "        assert probabilities is not None\n",
    "\n",
    "        return probabilities\n",
    "\n",
    "\n",
    "    def get_action(self, state):\n",
    "        \"\"\"\n",
    "        Compute the action to take in the current state, including exploration.\n",
    "        Select actions according to softmax policy.\n",
    "\n",
    "        Note: To pick randomly from a list, use np.random.choice(..., p=actions_probabilities)\n",
    "              To pick True or False with a given probablity, generate uniform number in [0, 1]\n",
    "              and compare it with your probability\n",
    "        \"\"\"\n",
    "        possible_actions = self.get_legal_actions(state)\n",
    "        # If there are no legal actions, return None\n",
    "        if len(possible_actions) == 0:\n",
    "            return None\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        # Select the action to take in the current state according to the policy\n",
    "        chosen_action = None\n",
    "        assert chosen_action is not None\n",
    "        return chosen_action"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
